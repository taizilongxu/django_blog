##简介

支持向量机(Support Vector Machine)是Cortes和Vapnik于1995年首先提出的，它在解决__小样本__[^1]、__非线性__[^2]及__高维模式识别__[^3]中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。

支持向量机方法是建立在统计学习理论的__VC维__[^4]理论和__结构风险最小__原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力（或称泛化能力）。

##结构风险

模型与真实值之间的误差叫做__风险__

使用分类器在样本数据上的分类的结果与真实结果之间的差值来表示。这个差值叫做__经验风险__，在骁样本上可以保证没有误差，但是真实值之间不可能保证

以前的机器学习方法都把经验风险最小化作为努力的目标，但后来发现很多分类函数能够在样本集上轻易达到100%的正确率，在真实分类时却一塌糊涂，即所谓的__推广能力差__，或__泛化能力差__

__泛化误差界__，由两部分刻画：

* __经验风险__，代表了分类器在给定样本上的误差
* __置信风险__，代表了我们在多大程度上可以信任分类器在未知文本上分类的结果
    * __样本数量__，显然给定的样本数量越大，我们的学习结果越有可能正确，此时置信风险越小
    * 分类函数的__VC维__，显然VC维越大，推广能力越差，置信风险会变大

泛化误差界的公式为：

    R(w)≤Remp(w)+Ф(n/h)

公式中R(w)就是真实风险，Remp(w)就是经验风险，Ф(n/h)就是置信风险。
统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。
SVM正是这样一种努力最小化结构风险的算法。
​
##VC维
__定义__：对一个指标函数集，如果存在H个样本能够被函数集中的函数按所有可能的2的H次方种形式分开，则称函数集能够把H个样本打散；函数集的VC维就是它能打散的最大样本数目H。​

二维：VC维为3
1​​

VC维反映了函数集的学习能力，VC维越大则学习机器越复杂（容量越大），遗憾的是，目前尚没有通用的关于任意函数集VC维计算的理论，只对一些特殊的函数集知道其VC维。例如在N维空间中线形分类器和线形实函数的VC维是N+1。（通过2维的推导）




​
[^1] : 并不是说样本的绝对数量少（实际上，对任何算法来说，更多的样本几乎总是能带来更好的效果），而是说与问题的复杂度比起来，SVM算法要求的样本数是相对比较少的。
[^2] : 是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现，这一部分是SVM的精髓，以后会详细讨论。多说一句，关于文本分类这个问题究竟是不是线性可分的，尚没有定论，因此不能简单的认为它是线性可分的而作简化处理，在水落石出之前，只好先当它是线性不可分的（反正线性可分也不过是线性不可分的一种特例而已，我们向来不怕方法过于通用）。
[^3] : 高维模式识别是指样本维数很高，例如文本的向量表示，如果没有经过另一系列文章（《文本分类入门》）中提到过的降维处理，出现几万维的情况很正常，其他算法基本就没有能力应付了，SVM却可以，主要是因为SVM 产生的分类器很简洁，用到的样本信息很少（仅仅用到那些称之为“支持向量”的样本，此为后话），使得即使样本维数很高，也不会给存储和计算带来大麻烦
[^4] : VC维是对函数类的一种度量，可以简单的理解为问题的复杂程度，VC维越高，一个问题就越复杂。
